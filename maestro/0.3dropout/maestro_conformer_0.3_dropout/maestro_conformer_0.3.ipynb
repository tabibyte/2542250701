{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3da3896-1e02-43b6-9ef9-18872cf278bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q miditok symusic tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92e4207-f753-4fc0-878a-7c5bcd5bec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from symusic import Score\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "020e3312-fd31-472b-9572-426a289be629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPUs available: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901116bd-b1d1-4234-9be5-0d61f9db2d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    DATA_DIR = \"/workspace/MAESTRO\"\n",
    "    \n",
    "    FILE_LIMIT = None     \n",
    "    STRIDE = 256         \n",
    "    SEQ_LENGTH = 1024     \n",
    "    \n",
    "    EMBED_DIM = 512     \n",
    "    N_HEADS = 8\n",
    "    N_LAYERS = 8\n",
    "    CNN_KERNEL = 65       \n",
    "    DROPOUT = 0.3      \n",
    "    \n",
    "    BATCH_SIZE = 64\n",
    "    ACCUMULATION_STEPS = 2\n",
    "    LEARNING_RATE = 1e-3  \n",
    "    EPOCHS = 50           \n",
    "    PATIENCE = 5          \n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "print(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79c9413-67f8-4d6b-8d2d-0767d57157bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning /workspace/MAESTRO...\n",
      "Train Files: 1148 | Val Files: 128\n",
      "Processing Training Set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efa27a0971640149dc59d4a1fb065e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Tokenization:   0%|          | 0/1148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Validation Set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a241caddc0e448af80275e476a86f03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Tokenization:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_split_maestro(path, split_ratio=0.9):\n",
    "    print(f\"Scanning {path}...\")\n",
    "    # 1. Get all filenames first\n",
    "    files = list(Path(path).glob(\"**/*.midi\")) + list(Path(path).glob(\"**/*.mid\"))\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    # 2. Split the FILES, not the tokens\n",
    "    split_idx = int(len(files) * split_ratio)\n",
    "    train_files = files[:split_idx]\n",
    "    val_files = files[split_idx:]\n",
    "    \n",
    "    print(f\"Train Files: {len(train_files)} | Val Files: {len(val_files)}\")\n",
    "    \n",
    "    # 3. Helper to tokenize a specific list of files\n",
    "    def tokenize_files(file_list, desc):\n",
    "        tokenizer_config = TokenizerConfig(num_velocities=16, use_chords=True, use_tempos=True)\n",
    "        tokenizer = REMI(tokenizer_config)\n",
    "        tokens_list = []\n",
    "        for f in tqdm(file_list, desc=desc):\n",
    "            try:\n",
    "                midi = Score(f)\n",
    "                toks = tokenizer(midi)\n",
    "                if len(toks) > 0: tokens_list.extend(toks[0].ids)\n",
    "            except: pass\n",
    "        return torch.tensor(tokens_list, dtype=torch.long), tokenizer\n",
    "\n",
    "    print(\"Processing Training Set...\")\n",
    "    train_data, tokenizer = tokenize_files(train_files, \"Train Tokenization\")\n",
    "    \n",
    "    print(\"Processing Validation Set...\")\n",
    "    val_data, _ = tokenize_files(val_files, \"Val Tokenization\")\n",
    "    \n",
    "    return train_data, val_data, tokenizer\n",
    "\n",
    "train_tensor, val_tensor, tokenizer = load_split_maestro(config.DATA_DIR)\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d618faec-a2a5-430b-8c32-051a813e2d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 24 workers for data loading.\n",
      "Training Batches per Epoch: 1413\n"
     ]
    }
   ],
   "source": [
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, data_tensor, seq_len, stride):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = data_tensor\n",
    "        self.indices = range(0, len(data_tensor) - seq_len - 1, stride)\n",
    "\n",
    "    def __len__(self): return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.indices[idx]\n",
    "        # X: Context, Y: Target\n",
    "        return (self.data[start_idx : start_idx + self.seq_len], \n",
    "                self.data[start_idx+1 : start_idx + self.seq_len + 1])\n",
    "\n",
    "train_ds = MidiDataset(train_tensor, config.SEQ_LENGTH, config.STRIDE)\n",
    "val_ds = MidiDataset(val_tensor, config.SEQ_LENGTH, config.STRIDE)\n",
    "\n",
    "# Loaders\n",
    "\n",
    "import multiprocessing\n",
    "num_cpu = multiprocessing.cpu_count()\n",
    "workers = min(24, num_cpu)\n",
    "print(f\"Using {workers} workers for data loading.\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=workers,pin_memory=True,persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=workers,pin_memory=True,persistent_workers=True)\n",
    "\n",
    "print(f\"Training Batches per Epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726ae17b-4cc2-40df-9bb5-008fa471e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div)\n",
    "        pe[:, 1::2] = torch.cos(position * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x): return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd92a5d1-e1b4-47b1-b448-90998318417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalMacaronBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, cnn_k, dropout):\n",
    "        super().__init__()\n",
    "        self.cnn_k = cnn_k\n",
    "        \n",
    "        # 1. First Feed-Forward\n",
    "        self.ff1_norm = nn.LayerNorm(d_model)\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # 2. Multi-Head Self Attention\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # 3. Causal Convolution Module\n",
    "        self.conv_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Pointwise 1\n",
    "        self.conv_pointwise1 = nn.Conv1d(d_model, d_model * 2, 1)\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "        \n",
    "        # Depthwise Conv (NO PADDING defined here; we do it manually)\n",
    "        self.conv_depthwise = nn.Conv1d(d_model, d_model, cnn_k, padding=0, groups=d_model)\n",
    "        \n",
    "        # Norm & Pointwise 2\n",
    "        self.conv_batchnorm = nn.BatchNorm1d(d_model)\n",
    "        self.conv_act = nn.SiLU()\n",
    "        self.conv_pointwise2 = nn.Conv1d(d_model, d_model, 1)\n",
    "        self.conv_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 4. Second Feed-Forward\n",
    "        self.ff2_norm = nn.LayerNorm(d_model)\n",
    "        self.ff2 = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 1. First FFN\n",
    "        x = x + 0.5 * self.ff1(self.ff1_norm(x))\n",
    "        \n",
    "        # 2. Attention\n",
    "        res = x\n",
    "        x_norm = self.attn_norm(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=mask, is_causal=True)\n",
    "        x = res + attn_out\n",
    "        \n",
    "        # 3. Causal Convolution\n",
    "        res = x\n",
    "        x_cnn = self.conv_norm(x).permute(0, 2, 1) # [B, Dim, Seq]\n",
    "        \n",
    "        # Expansion\n",
    "        x_cnn = self.conv_pointwise1(x_cnn)\n",
    "        x_cnn = self.glu(x_cnn)\n",
    "        \n",
    "        x_cnn = F.pad(x_cnn, (self.cnn_k - 1, 0))\n",
    "        \n",
    "        x_cnn = self.conv_depthwise(x_cnn)\n",
    "        x_cnn = self.conv_batchnorm(x_cnn)\n",
    "        x_cnn = self.conv_act(x_cnn)\n",
    "        x_cnn = self.conv_pointwise2(x_cnn)\n",
    "        x_cnn = self.conv_dropout(x_cnn)\n",
    "        \n",
    "        x = res + x_cnn.permute(0, 2, 1)\n",
    "        \n",
    "        # 4. Second FFN\n",
    "        x = x + 0.5 * self.ff2(self.ff2_norm(x))\n",
    "        \n",
    "        return self.final_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61b1e42-3953-4a59-8108-02082108ec3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "Model Parameters: 48,945,466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_244012/511415585.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "class MusicConformer(nn.Module):\n",
    "    def __init__(self, vocab_size, cfg):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, cfg.EMBED_DIM)\n",
    "        self.pos = PositionalEncoding(cfg.EMBED_DIM)\n",
    "        self.layers = nn.ModuleList([\n",
    "            CausalMacaronBlock(cfg.EMBED_DIM, cfg.N_HEADS, cfg.CNN_KERNEL, cfg.DROPOUT) \n",
    "            for _ in range(cfg.N_LAYERS)\n",
    "        ])\n",
    "        self.head = nn.Linear(cfg.EMBED_DIM, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = torch.triu(torch.ones(x.size(1), x.size(1)) * float('-inf'), diagonal=1).to(x.device)\n",
    "        x = self.pos(self.embed(x))\n",
    "        for layer in self.layers: \n",
    "            x = layer(x, mask)\n",
    "        return self.head(x)\n",
    "\n",
    "model = MusicConformer(vocab_size, config)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(config.DEVICE)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=config.LEARNING_RATE,\n",
    "    steps_per_epoch=len(train_loader) // config.ACCUMULATION_STEPS,\n",
    "    epochs=config.EPOCHS,\n",
    "    pct_start=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dbe33a3-e301-4080-bafb-84d1f6baefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001, path='best_conformer.pth'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            print(f\"Saved (Loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement ({self.counter}/{self.patience})\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_epoch(model, loader, opt, scaler, sched):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    loop = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for i, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "        \n",
    "        with autocast():\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            loss = loss / config.ACCUMULATION_STEPS \n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Accuracy\n",
    "        predictions = logits.argmax(dim=-1) \n",
    "        total_correct += (predictions == y).sum().item()\n",
    "        total_samples += y.numel()\n",
    "        \n",
    "        # Step only every N batches\n",
    "        if (i + 1) % config.ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            sched.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Metrics for display\n",
    "        current_loss = loss.item() * config.ACCUMULATION_STEPS\n",
    "        total_loss += current_loss\n",
    "        loop.set_postfix(loss=f\"{current_loss:.4f}\", acc=f\"{total_correct/total_samples:.2%}\")\n",
    "        \n",
    "    return total_loss / len(loader), total_correct / total_samples\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "            with autocast():\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            \n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            total_correct += (predictions == y).sum().item()\n",
    "            total_samples += y.numel()\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader), total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e28f8d7f-8369-4f1c-902d-9158593fd9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001, path='best_conformer.pth'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            print(f\"Saved (Loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"No improvement ({self.counter}/{self.patience})\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "def train_epoch(model, loader, opt, scaler, sched):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    loop = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for i, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "        \n",
    "        with autocast():\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            loss = loss / config.ACCUMULATION_STEPS \n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Accuracy\n",
    "        predictions = logits.argmax(dim=-1) \n",
    "        total_correct += (predictions == y).sum().item()\n",
    "        total_samples += y.numel()\n",
    "        \n",
    "        # Step only every N batches\n",
    "        if (i + 1) % config.ACCUMULATION_STEPS == 0:\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            sched.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Metrics for display\n",
    "        current_loss = loss.item() * config.ACCUMULATION_STEPS\n",
    "        total_loss += current_loss\n",
    "        loop.set_postfix(loss=f\"{current_loss:.4f}\", acc=f\"{total_correct/total_samples:.2%}\")\n",
    "        \n",
    "    return total_loss / len(loader), total_correct / total_samples\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "            with autocast():\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            \n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            total_correct += (predictions == y).sum().item()\n",
    "            total_samples += y.numel()\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader), total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e6d82df-d56a-40fa-9cb2-36973cbcc6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85611d9d3194504b08d09c29e2cb336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_244012/2404826671.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipykernel_244012/2404826671.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 2.6813 (Acc: 28.20%) | Val Loss: 2.2577 (Acc: 32.69%) | PPL: 9.56\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb7abec93ec4ad49ce709f9ae6ae62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train Loss: 2.0862 (Acc: 36.17%) | Val Loss: 1.9361 (Acc: 38.98%) | PPL: 6.93\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8370ff5584764d8fa47f692a4fa81582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train Loss: 1.7998 (Acc: 42.64%) | Val Loss: 1.6914 (Acc: 45.02%) | PPL: 5.43\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b7295272194fe2bb0bf99f18948b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Train Loss: 1.6111 (Acc: 47.24%) | Val Loss: 1.5629 (Acc: 48.37%) | PPL: 4.77\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c02f16d4e94212bdfa42d351600f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Train Loss: 1.4751 (Acc: 50.59%) | Val Loss: 1.4688 (Acc: 50.50%) | PPL: 4.34\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155b917bc0934ca2803f1c5b9f6c80bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Train Loss: 1.3711 (Acc: 53.12%) | Val Loss: 1.4205 (Acc: 51.68%) | PPL: 4.14\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0934bec5f4d74ad98f8899f9ac214797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Train Loss: 1.2947 (Acc: 54.99%) | Val Loss: 1.3962 (Acc: 52.46%) | PPL: 4.04\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a986259f051146a0949db707a4a0319c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Train Loss: 1.2367 (Acc: 56.44%) | Val Loss: 1.3737 (Acc: 53.11%) | PPL: 3.95\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907b04fd35c84f2689bf31fa469e85e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Train Loss: 1.1906 (Acc: 57.62%) | Val Loss: 1.3655 (Acc: 53.37%) | PPL: 3.92\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0d8ce9a4454847a1c3ea23c5397d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Train Loss: 1.1521 (Acc: 58.63%) | Val Loss: 1.3641 (Acc: 53.51%) | PPL: 3.91\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8c4040896d4ccb801586a2cccd6b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Train Loss: 1.1191 (Acc: 59.52%) | Val Loss: 1.3739 (Acc: 53.66%) | PPL: 3.95\n",
      "No improvement (1/5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b43ff4796b34693a6b2d17c9dcb97b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Train Loss: 1.0901 (Acc: 60.31%) | Val Loss: 1.3685 (Acc: 53.71%) | PPL: 3.93\n",
      "No improvement (2/5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974d24bda1ae4e3d8bebb250bf4ef25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Train Loss: 1.0638 (Acc: 61.06%) | Val Loss: 1.3754 (Acc: 53.80%) | PPL: 3.96\n",
      "No improvement (3/5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7319280d424e5fa81faac09fd960da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Train Loss: 1.0400 (Acc: 61.75%) | Val Loss: 1.3797 (Acc: 53.82%) | PPL: 3.97\n",
      "No improvement (4/5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7828e7068c47db9687f87c17b0ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1413 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Train Loss: 1.0184 (Acc: 62.39%) | Val Loss: 1.3805 (Acc: 53.83%) | PPL: 3.98\n",
      "No improvement (5/5)\n",
      "Early stopping triggered.\n"
     ]
    },
    {

    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scaler, scheduler)\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    \n",
    "    try: ppl = math.exp(val_loss)\n",
    "    except: ppl = float('inf')\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{config.EPOCHS} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} (Acc: {train_acc:.2%}) | \"\n",
    "        f\"Val Loss: {val_loss:.4f} (Acc: {val_acc:.2%}) | \"\n",
    "        f\"PPL: {ppl:.2f}\"\n",
    "    )\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict(), \"conformer_0.3.pth\")\n",
    "        print(\"Saved New Best Model\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement ({patience_counter}/{config.PATIENCE})\")\n",
    "        if patience_counter >= config.PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best weights\n",
    "model.load_state_dict(torch.load(\"best_macaron_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b38df139-7482-4f40-bda6-6086b32dc51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Generation Loop...\n",
      "Generating... (Target: 1024 tokens)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38ce8a7fa2e43ea924b9be5d5c5a1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: conformer_0.3_1_065600.mid\n",
      "Generating... (Target: 1024 tokens)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a84bbab2a14b7a97f48b9aaf1d2312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: conformer_0.3_2_065608.mid\n",
      "Generating... (Target: 1024 tokens)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4efa9da0c484c84850e1335d730dcee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: conformer_0.3_3_065616.mid\n",
      "Generating... (Target: 1024 tokens)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13c4f49c0314af8a4f5230fb4e5548f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: conformer_0.3_4_065625.mid\n",
      "Generating... (Target: 1024 tokens)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccc86baf9e142c1a6949ec062536775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: conformer_0.3_5_065633.mid\n"
     ]
    }
   ],
   "source": [
    "from miditok import TokSequence\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# 1. Configuration\n",
    "GEN_CONF = {\n",
    "    'num_examples': 5,\n",
    "    'max_len': 1024,       \n",
    "    'temperature': 0.8,\n",
    "    'top_k': 20\n",
    "}\n",
    "\n",
    "# 2. Generation Function\n",
    "def generate_sequence(model, tokenizer, length, temp=1.0, top_k=20):\n",
    "    model.eval()\n",
    "    # Seed with a random token from the vocab\n",
    "    start_token = torch.randint(0, tokenizer.vocab_size, (1, 1)).to(config.DEVICE)\n",
    "    generated = start_token\n",
    "    \n",
    "    print(f\"Generating... (Target: {length} tokens)\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(length), desc=\"Gen\"):\n",
    "            logits = model(generated)\n",
    "            # Focus on the last token\n",
    "            last_logits = logits[:, -1, :] / temp\n",
    "            \n",
    "            # Top-K Sampling (Filters out bad notes)\n",
    "            v, _ = torch.topk(last_logits, top_k)\n",
    "            last_logits[last_logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(last_logits, dim=-1)\n",
    "            \n",
    "            # Sample\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            \n",
    "    return generated[0].cpu().numpy()\n",
    "\n",
    "# 3. Main Loop\n",
    "print(\"Starting Generation Loop...\")\n",
    "\n",
    "for i in range(GEN_CONF['num_examples']):\n",
    "    try:\n",
    "        # A. Generate\n",
    "        raw_tokens = generate_sequence(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            GEN_CONF['max_len'], \n",
    "            GEN_CONF['temperature'], \n",
    "            GEN_CONF['top_k']\n",
    "        )\n",
    "        \n",
    "        # B. Convert to Python List\n",
    "        token_list = raw_tokens.tolist()\n",
    "        \n",
    "        # C. Wrap in TokSequence\n",
    "        seq = TokSequence(ids=token_list)\n",
    "        \n",
    "        # D. Decode and Save\n",
    "        generated_score = tokenizer.decode([seq])\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%H%M%S\")\n",
    "        filename = f\"conformer_0.3_{i+1}_{timestamp}.mid\"\n",
    "        generated_score.dump_midi(filename)\n",
    "        print(f\"Saved: {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error on file {i+1}: {e}\")\n",
    "        if 'token_list' in locals():\n",
    "            print(f\"First 10 tokens: {token_list[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db132ca8-712e-4936-b351-39d06c3019c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
