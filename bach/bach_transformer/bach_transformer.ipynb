{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8b9817-0166-4561-8d40-6efaae0dfab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q miditok symusic tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03188953-fb4c-4a0d-b7c8-7c42223dfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from symusic import Score\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cb6664-7dfa-4c15-98c3-4fa2cff4b708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    DATA_DIR = \"BACH_MIDI\"\n",
    "\n",
    "    STRIDE = 128           \n",
    "    SEQ_LENGTH = 1024      \n",
    "    \n",
    "    EMBED_DIM = 256\n",
    "    N_HEADS = 8\n",
    "    N_LAYERS = 6\n",
    "    DROPOUT = 0.3          \n",
    "    \n",
    "    BATCH_SIZE = 16\n",
    "    ACCUMULATION_STEPS = 1 \n",
    "    LEARNING_RATE = 5e-4\n",
    "    EPOCHS = 50\n",
    "    PATIENCE = 5\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "print(f\"Device: {config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1217e87-9ca6-43ed-a079-601f4c30e581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 229 Train | 76 Valid files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7fd8bd042949e4a0dce0cb28a22365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Train:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0dc4afa2d44bc2946c9f6a663a5a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Valid:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 202\n"
     ]
    }
   ],
   "source": [
    "def load_bach_remi(path):\n",
    "    path = Path(path)\n",
    "    train_files = list((path / \"train\").glob(\"*.mid\"))\n",
    "    val_files = list((path / \"valid\").glob(\"*.mid\"))\n",
    "    \n",
    "    print(f\"Found {len(train_files)} Train | {len(val_files)} Valid files\")\n",
    "\n",
    "    tokenizer_config = TokenizerConfig(\n",
    "        num_velocities=1,      \n",
    "        use_chords=True,      \n",
    "        use_tempos=False,    \n",
    "        use_velocities=False, \n",
    "        use_durations=False,  \n",
    "        use_programs=False,\n",
    "        beat_res={(0, 4): 4}   \n",
    "    )\n",
    "    tokenizer = REMI(tokenizer_config)\n",
    "    \n",
    "    def tokenize_batch(file_list, desc):\n",
    "        tokens_list = []\n",
    "        for f in tqdm(file_list, desc=desc):\n",
    "            try:\n",
    "                midi = Score(f)\n",
    "                toks = tokenizer(midi)\n",
    "                if len(toks) > 0: tokens_list.extend(toks[0].ids)\n",
    "            except: pass\n",
    "        return torch.tensor(tokens_list, dtype=torch.long)\n",
    "\n",
    "    train_tensor = tokenize_batch(train_files, \"Tokenizing Train\")\n",
    "    val_tensor = tokenize_batch(val_files, \"Tokenizing Valid\")\n",
    "    \n",
    "    return train_tensor, val_tensor, tokenizer\n",
    "\n",
    "train_tensor, val_tensor, tokenizer = load_bach_remi(config.DATA_DIR)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"Vocab Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28d21a7-6d88-4fd4-8469-96ece5fe1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, data_tensor, seq_len, stride):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = data_tensor\n",
    "        self.indices = range(0, len(data_tensor) - seq_len - 1, stride)\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.indices[idx]\n",
    "        return (self.data[start_idx : start_idx + self.seq_len], \n",
    "                self.data[start_idx+1 : start_idx + self.seq_len + 1])\n",
    "\n",
    "train_ds = MidiDataset(train_tensor, config.SEQ_LENGTH, config.STRIDE)\n",
    "val_ds = MidiDataset(val_tensor, config.SEQ_LENGTH, config.STRIDE)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80985b95-7815-453c-9f1d-5fa0bd276266",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div)\n",
    "        pe[:, 1::2] = torch.cos(position * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x): return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class StandardBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        # 1. Self Attention\n",
    "        self.attn_norm = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # 2. Feed-Forward\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        res = x\n",
    "        x_norm = self.attn_norm(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=mask, is_causal=True)\n",
    "        x = res + attn_out\n",
    "        \n",
    "        res = x\n",
    "        ffn_out = self.ffn(self.ffn_norm(x))\n",
    "        x = res + ffn_out\n",
    "        return x\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, cfg):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, cfg.EMBED_DIM)\n",
    "        self.pos = PositionalEncoding(cfg.EMBED_DIM)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            StandardBlock(cfg.EMBED_DIM, cfg.N_HEADS, cfg.DROPOUT) \n",
    "            for _ in range(cfg.N_LAYERS)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(cfg.EMBED_DIM)\n",
    "        self.head = nn.Linear(cfg.EMBED_DIM, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Causal Mask\n",
    "        mask = torch.triu(torch.ones(x.size(1), x.size(1)) * float('-inf'), diagonal=1).to(x.device)\n",
    "        \n",
    "        x = self.pos(self.embed(x))\n",
    "        for layer in self.layers: \n",
    "            x = layer(x, mask)\n",
    "        x = self.final_norm(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "578549ae-e4a1-4afd-b4ae-3550b101b05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 4,842,698\n"
     ]
    }
   ],
   "source": [
    "model = MusicTransformer(vocab_size, config).to(config.DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=config.LEARNING_RATE,\n",
    "    steps_per_epoch=len(train_loader), epochs=config.EPOCHS, pct_start=0.1\n",
    ")\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    loop = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    \n",
    "    for i, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "        \n",
    "        with autocast():\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Accuracy Metrics\n",
    "        predictions = logits.argmax(dim=-1) \n",
    "        total_correct += (predictions == y).sum().item()\n",
    "        total_samples += y.numel()\n",
    "        \n",
    "        if (i + 1) % config.ACCUMULATION_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "    return total_loss / len(loader), total_correct / total_samples\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "            with autocast():\n",
    "                logits = model(x)\n",
    "                loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "            \n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            total_correct += (predictions == y).sum().item()\n",
    "            total_samples += y.numel()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(loader), total_correct / total_samples\n",
    "\n",
    "print(f\"Model Params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6efb687-b2d8-4bae-b44b-fb700e3a73a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bf9896fb2d4378940b437f44708dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9270/2966222310.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipykernel_9270/2966222310.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train: 2.6500 (46.0%) | Val: 1.9901 (50.0%) | PPL: 7.32\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc7d38b01714d6f8c3ffacb1ea99fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Train: 1.8660 (51.6%) | Val: 1.6767 (55.4%) | PPL: 5.35\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1547200472ff45b5837650cb04c59867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Train: 1.4803 (59.0%) | Val: 1.1930 (62.9%) | PPL: 3.30\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c5574a839a4931b6717f76a7bc63e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Train: 0.8020 (75.7%) | Val: 0.3494 (90.1%) | PPL: 1.42\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e784f49eff649ad9443915f0da8b84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Train: 0.3322 (90.2%) | Val: 0.2610 (92.2%) | PPL: 1.30\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1f2eb5b5124793b8e93e7e1ec6245b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Train: 0.2578 (92.1%) | Val: 0.2406 (92.8%) | PPL: 1.27\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0e352e1f8a4db19d9ab821c4b4ec28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Train: 0.2267 (93.0%) | Val: 0.2303 (93.1%) | PPL: 1.26\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf7fb1ae7b94851a07dbdb1ad1e6b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Train: 0.2064 (93.5%) | Val: 0.2247 (93.3%) | PPL: 1.25\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed145e4a55c4865b0470df2cd448e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Train: 0.1892 (93.9%) | Val: 0.2231 (93.3%) | PPL: 1.25\n",
      "Saved New Best Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249d8c762d824786a25d1e1b4ce32cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Train: 0.1728 (94.4%) | Val: 0.2248 (93.3%) | PPL: 1.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5617caa6c0f94ea488a5ccd1c62b8f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Train: 0.1579 (94.8%) | Val: 0.2314 (93.3%) | PPL: 1.26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a300bcea3eca471e954c3261bc909308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Train: 0.1424 (95.3%) | Val: 0.2408 (93.2%) | PPL: 1.27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38453342458a448680187f3e5fbf9afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Train: 0.1266 (95.8%) | Val: 0.2534 (93.1%) | PPL: 1.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe6bc74928a46048475bd5939e83601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Train: 0.1115 (96.2%) | Val: 0.2692 (92.8%) | PPL: 1.31\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    t_loss, t_acc = train_epoch(model, train_loader)\n",
    "    v_loss, v_acc = validate(model, val_loader)\n",
    "    \n",
    "    try: ppl = math.exp(v_loss)\n",
    "    except: ppl = float('inf')\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{config.EPOCHS} | \"\n",
    "        f\"Train: {t_loss:.4f} ({t_acc:.1%}) | \"\n",
    "        f\"Val: {v_loss:.4f} ({v_acc:.1%}) | \"\n",
    "        f\"PPL: {ppl:.2f}\"\n",
    "    )\n",
    "    \n",
    "    if v_loss < best_loss:\n",
    "        best_loss = v_loss\n",
    "        patience = 0\n",
    "        torch.save(model.state_dict(), \"best_bach_transformer.pth\")\n",
    "        print(\"Saved New Best Model\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= config.PATIENCE:\n",
    "            print(\"Early Stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e9fe552-182d-469a-8f9b-72951b29da38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating... (Target: 1024)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadffffc5a2141bbb662122387782ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: bach_transformer_1.mid\n",
      "Generating... (Target: 1024)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8830bdf34494c038d402dfd3c896ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Gen:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: bach_transformer_2.mid\n"
     ]
    }
   ],
   "source": [
    "from miditok import TokSequence\n",
    "from datetime import datetime\n",
    "\n",
    "GEN_CONF = {\n",
    "    'num_examples': 2,\n",
    "    'max_len': 1024,\n",
    "    'temperature': 0.9,\n",
    "    'top_k': 10\n",
    "}\n",
    "\n",
    "def generate_sequence(model, tokenizer, length, temp=1.0, top_k=20):\n",
    "    model.eval()\n",
    "    start_token = torch.randint(0, tokenizer.vocab_size, (1, 1)).to(config.DEVICE)\n",
    "    generated = start_token\n",
    "    \n",
    "    print(f\"Generating... (Target: {length})\")\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(length), desc=\"Gen\"):\n",
    "            logits = model(generated)\n",
    "            last_logits = logits[:, -1, :] / temp\n",
    "            v, _ = torch.topk(last_logits, top_k)\n",
    "            last_logits[last_logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(last_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated[0].cpu().numpy()\n",
    "\n",
    "for i in range(GEN_CONF['num_examples']):\n",
    "    try:\n",
    "        raw_tokens = generate_sequence(model, tokenizer, GEN_CONF['max_len'], GEN_CONF['temperature'], GEN_CONF['top_k'])\n",
    "        seq = TokSequence(ids=raw_tokens.tolist())\n",
    "        score = tokenizer.decode([seq])\n",
    "        filename = f\"bach_transformer_{i+1}.mid\"\n",
    "        score.dump_midi(filename)\n",
    "        print(f\"Saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb6c14-086b-4464-be7d-b2ed901e6f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
